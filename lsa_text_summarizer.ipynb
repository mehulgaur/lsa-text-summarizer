{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Text Summarization refers to the technique of  shortening long pieces of text. The intention is to create a coherent and fluent summary having only the main points outlined in the document.\n",
    "\n",
    "We cannot possibly create summaries of all of the text manually, there is a great need for automatic methods\n",
    "\n",
    "Automatic text summarization is a common problem in machine learning and natural language processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import numpy\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from numpy.linalg import svd as singular_value_decomposition\n",
    "from nltk.corpus import stopwords\n",
    "from warnings import warn\n",
    "from operator import attrgetter\n",
    "from collections import namedtuple\n",
    "from nltk.corpus import stopwords\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Source File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file = \"original_text.txt\"\n",
    "\n",
    "with open(source_file, \"r\", encoding='utf-8') as file:\n",
    "    text = file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemsCount(object):\n",
    "    def __init__(self, value):\n",
    "        self._value = value\n",
    "\n",
    "    def __call__(self, sequence):\n",
    "        if isinstance(self._value, (bytes, str,)):\n",
    "            if self._value.endswith(\"%\"):\n",
    "                total_count = len(sequence)\n",
    "                percentage = int(self._value[:-1])\n",
    "                # at least one sentence should be chosen\n",
    "                count = max(1, total_count*percentage // 100)\n",
    "                return sequence[:count]\n",
    "            else:\n",
    "                return sequence[:int(self._value)]\n",
    "        elif isinstance(self._value, (int, float)):\n",
    "            return sequence[:int(self._value)]\n",
    "        else:\n",
    "            ValueError(\"Unsuported value of items count '%s'.\" % self._value)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return to_string(\"<ItemsCount: %r>\" % self._value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentenceInfo = namedtuple(\"SentenceInfo\", (\"sentence\", \"order\", \"rating\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSummarizer(object):\n",
    "    \n",
    "    def __call__(self, document, sentences_count):\n",
    "        raise NotImplementedError(\"This method should be overriden in subclass\")\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_word(word):\n",
    "        return word.lower()\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_best_sentences(sentences, count, rating, *args, **kwargs):\n",
    "        rate = rating\n",
    "        if isinstance(rating, dict):\n",
    "            assert not args and not kwargs\n",
    "            rate = lambda s: rating[s]\n",
    "\n",
    "        infos = (SentenceInfo(s, o, rate(s, *args, **kwargs))\n",
    "            for o, s in enumerate(sentences))\n",
    "\n",
    "        # sort sentences by rating in descending order\n",
    "        infos = sorted(infos, key=attrgetter(\"rating\"), reverse=True)\n",
    "        # get `count` first best rated sentences\n",
    "        if not isinstance(count, ItemsCount):\n",
    "            count = ItemsCount(count)\n",
    "        infos = count(infos)\n",
    "        # sort sentences by their order in document\n",
    "        infos = sorted(infos, key=attrgetter(\"order\"))\n",
    "\n",
    "        return tuple(i.sentence for i in infos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA\n",
    "Latent semantic analysis method extracts hidden semantic structure of words and sentences. LSA uses the context of the input document and extracts information such as which words are used together and which common words are seen in different sentences.\n",
    "\n",
    "###  Process\n",
    "1. Sentence Tokenization\n",
    "2. Word Tokenization\n",
    "3. Removing Stop Word\n",
    "4. Input matrix creation\n",
    "5. Compute Term Frequency\n",
    "6. Rank The Sentaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LsaSummarizer(BaseSummarizer):\n",
    "    MIN_DIMENSIONS = 3\n",
    "    REDUCTION_RATIO = 1/1\n",
    "\n",
    "    _stop_words = list(stopwords.words('english'))\n",
    "\n",
    "    @property\n",
    "    def stop_words(self):\n",
    "        return self._stop_words\n",
    "\n",
    "    @stop_words.setter\n",
    "    def stop_words(self, words):\n",
    "        self._stop_words = words\n",
    "\n",
    "    def __call__(self, document, sentences_count):\n",
    "\n",
    "        dictionary = self._create_dictionary(document)\n",
    "        \n",
    "        if not dictionary:\n",
    "            return ()\n",
    "\n",
    "    ##Tokenize paragraph into individual senteneces\n",
    "        sentences = sent_tokenize(document)\n",
    "\n",
    "        matrix = self._create_matrix(document, dictionary)\n",
    "        matrix = self._compute_term_frequency(matrix)\n",
    "        u, sigma, v = singular_value_decomposition(matrix, full_matrices=False)\n",
    "\n",
    "        ranks = iter(self._compute_ranks(sigma, v))\n",
    "        return self._get_best_sentences(sentences, sentences_count,\n",
    "            lambda s: next(ranks))\n",
    "\n",
    "    def _create_dictionary(self, document):\n",
    "        \"\"\"Creates mapping key = word, value = row index\"\"\"\n",
    "\n",
    "    ##Tokenize sentences into individual words\n",
    "        words = word_tokenize(document)\n",
    "        words = tuple(words)\n",
    "\n",
    "        words = map(self.normalize_word, words)\n",
    "\n",
    "        unique_words = frozenset(w for w in words if w not in self._stop_words)\n",
    "\n",
    "        return dict((w, i) for i, w in enumerate(unique_words))\n",
    "\n",
    "    def _create_matrix(self, document, dictionary):\n",
    "        \"\"\"\n",
    "        Creates matrix of shape where cells\n",
    "        contains number of occurences of words (rows) in senteces (cols).\n",
    "        \"\"\"\n",
    "        sentences = sent_tokenize(document)\n",
    "        words_count = len(dictionary)\n",
    "        sentences_count = len(sentences)\n",
    "        if words_count < sentences_count:\n",
    "            message = (\n",
    "                \"Number of words (%d) is lower than number of sentences (%d). \"\n",
    "                \"LSA algorithm may not work properly.\"\n",
    "            )\n",
    "            warn(message % (words_count, sentences_count))\n",
    "\n",
    "        matrix = numpy.zeros((words_count, sentences_count))\n",
    "        for col, sentence in enumerate(sentences):\n",
    "            words = word_tokenize(sentence)\n",
    "            for word in words:\n",
    "                # only valid words is counted (not stop-words, ...)\n",
    "                if word in dictionary:\n",
    "                    row = dictionary[word]\n",
    "                    matrix[row, col] += 1\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def _compute_term_frequency(self, matrix, smooth=0.4):\n",
    "        \"\"\"\n",
    "        Computes TF metrics for each sentence (column) in the given matrix and  normalize \n",
    "        the tf weights of all terms occurring in a document by the maximum tf in that document \n",
    "        \"\"\"\n",
    "        assert 0.0 <= smooth < 1.0\n",
    "\n",
    "        max_word_frequencies = numpy.max(matrix, axis=0)\n",
    "        rows, cols = matrix.shape\n",
    "        for row in range(rows):\n",
    "            for col in range(cols):\n",
    "                max_word_frequency = max_word_frequencies[col]\n",
    "                if max_word_frequency != 0:\n",
    "                    frequency = matrix[row, col]/max_word_frequency\n",
    "                    matrix[row, col] = smooth + (1.0 - smooth)*frequency\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def _compute_ranks(self, sigma, v_matrix):\n",
    "        assert len(sigma) == v_matrix.shape[0]\n",
    "\n",
    "        dimensions = max(LsaSummarizer.MIN_DIMENSIONS,\n",
    "            int(len(sigma)*LsaSummarizer.REDUCTION_RATIO))\n",
    "        powered_sigma = tuple(s**2 if i < dimensions else 0.0\n",
    "            for i, s in enumerate(sigma))\n",
    "\n",
    "        ranks = []\n",
    "        \n",
    "        for column_vector in v_matrix.T:\n",
    "            rank = sum(s*v**2 for s, v in zip(powered_sigma, column_vector))\n",
    "            ranks.append(math.sqrt(rank))\n",
    "\n",
    "        return ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Original text =====\n",
      "['As the days went on, and the special was released, and I sat through Dave’s storytelling and ranting on a fixation of now three different specials (trans women, and his feelings about them and their existence), I grew more and more annoyed. I’ve been a fan of Dave Chappelle since I saw Killin’ Them Softly, specifically the joke about Oscar the Grouch and how shitty everyone on Sesame Street treats him. I haven’t revisited that special in years, and I can still recite that entire act out in time with the cadence. I know it. It’s a profound and deeply hilarious moment that proved his genius in the pre-9/11 afterglow of the ’90s and I was always excited to discover his roles in movies before and after that.\\n', 'Anyway, I planned to scalp the tickets and not go, but I made that decision on Thursday thinking the show was Friday night. It wasn’t. With just three-and-a-half hours to show time, I decided to go. I knew it wasn’t going to be a stand-up set, or the Netflix special, so I was honestly curious about what it actually was. Was he gonna come out as trans? Was he gonna do a DJ set? I live in L.A.; you never really know.\\n', 'So as is customary for Chappelle events, they locked up our phones. I saw him at Radio City with Childish Gambino during his month of shows back in 2017, so I was familiar with the protocol. With some time to marinate, here’s what happened:\\n', 'Thundercat opened the “show” playing some beautiful songs on piano.\\n', 'A DJ (no disrespect, but I was waiting for a late friend to show up and dancing so I missed his name) played a bunch of throwback hits.\\n', 'Jeff Ross, “The Roastmaster” came out and did a pretty solid five minutes on his adopted German shepherds, and then decided to roast some volunteers from the audience, which, fine. But there was a guy in the front who was disabled, and a huge Chappelle fan, and Jeff went in on him…presumably as some sort of “fuck you” to “Twitter,” (aging contrarians’ new favorite boogeyman) but I’ll tell ya, in my kind of shitty seats, no one was laughing. I am super into section camaraderie, so we were cutting up in our little corner. It was age and race diverse; I sat behind a maybe 19-year-old Mexican kid vaping and next to a Southeast Asian couple in their early 40s, and there was a Black dude wearing a Dodgers hat across the way who was probably in his 50s. He had that guy-at-church-who’s-trying-to-get-his-catch-phrase-to-catch-on look about him. He just shouted random stuff, but again, we were all down for it. There was a White guy next to me, because someone was in his seat, but he moved when my friend got there… and we were all kind of like, “Why is he doing this?” It was like that Key & Peele sketch except the joke was not that Jeff didn’t wanna roast the man. I’m not sure what the joke was… cruelty?']\n",
      "====== End of original text =====\n",
      "\n",
      "========= Summary =========\n",
      "I’ve been a fan of Dave Chappelle since I saw Killin’ Them Softly, specifically the joke about Oscar the Grouch and how shitty everyone on Sesame Street treats him. I haven’t revisited that special in years, and I can still recite that entire act out in time with the cadence. It’s a profound and deeply hilarious moment that proved his genius in the pre-9/11 afterglow of the ’90s and I was always excited to discover his roles in movies before and after that.\n",
      "========= End of summary =========\n"
     ]
    }
   ],
   "source": [
    "summarizer = LsaSummarizer()\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "summarizer.stop_words = stopwords\n",
    "summary =summarizer(text[0], 3)\n",
    "\n",
    "print(\"====== Original text =====\")\n",
    "print(text)\n",
    "print(\"====== End of original text =====\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n========= Summary =========\")\n",
    "\n",
    "print(\" \".join(summary))\n",
    "print(\"========= End of summary =========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
